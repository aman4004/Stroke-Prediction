{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imb_pipeline\n",
    "\n",
    "# model building\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, roc_auc_score\n",
    "\n",
    "# other\n",
    "import joblib\n",
    "import warnings;warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>stroke</th>\n",
       "      <th>work_type_Govt_job</th>\n",
       "      <th>work_type_Never_worked</th>\n",
       "      <th>work_type_Private</th>\n",
       "      <th>work_type_Self-employed</th>\n",
       "      <th>work_type_children</th>\n",
       "      <th>smoking_status_Unknown</th>\n",
       "      <th>smoking_status_formerly smoked</th>\n",
       "      <th>smoking_status_never smoked</th>\n",
       "      <th>smoking_status_smokes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.662684</td>\n",
       "      <td>3.481240</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.249780</td>\n",
       "      <td>3.310543</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.547435</td>\n",
       "      <td>3.126761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.332705</td>\n",
       "      <td>3.335770</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.070223</td>\n",
       "      <td>3.186353</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease  ever_married  Residence_type  \\\n",
       "0       1  80.0             0              1             0               0   \n",
       "1       1  74.0             1              1             0               0   \n",
       "2       0  69.0             0              0             0               0   \n",
       "3       0  59.0             0              0             0               0   \n",
       "4       0  78.0             0              0             0               0   \n",
       "\n",
       "   avg_glucose_level       bmi  stroke  work_type_Govt_job  \\\n",
       "0           4.662684  3.481240       1                   0   \n",
       "1           4.249780  3.310543       1                   0   \n",
       "2           4.547435  3.126761       1                   0   \n",
       "3           4.332705  3.335770       1                   0   \n",
       "4           4.070223  3.186353       1                   0   \n",
       "\n",
       "   work_type_Never_worked  work_type_Private  work_type_Self-employed  \\\n",
       "0                       0                  1                        0   \n",
       "1                       0                  1                        0   \n",
       "2                       0                  1                        0   \n",
       "3                       0                  1                        0   \n",
       "4                       0                  1                        0   \n",
       "\n",
       "   work_type_children  smoking_status_Unknown  smoking_status_formerly smoked  \\\n",
       "0                   0                       0                               0   \n",
       "1                   0                       0                               0   \n",
       "2                   0                       0                               0   \n",
       "3                   0                       1                               0   \n",
       "4                   0                       1                               0   \n",
       "\n",
       "   smoking_status_never smoked  smoking_status_smokes  \n",
       "0                            1                      0  \n",
       "1                            1                      0  \n",
       "2                            1                      0  \n",
       "3                            0                      0  \n",
       "4                            0                      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('healthcare_preprocessed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with pipeline\n",
    "def models(trainx, testx, trainy, testy, parms=0, cm=False, predictor=-1, save=False):\n",
    "    \"\"\"This function takes train test split data pass it through a pipeline\n",
    "    consisting of selected models and return their respective score.\n",
    "    \n",
    "    Parameter\n",
    "    ----------------------------\n",
    "    trainx, trainy, testx, testy: x_train, y_train, x_test, y_test\n",
    "    params: Type Dict -> parameter for model, {applicable only when predictor is not equal to -1\n",
    "    cm: plot confusion matrix, default set to false\n",
    "    save: save a pickle file, default set to false\n",
    "    predictor: Default=-1 {-1: all, 0: Logistic Regression, 1: Decision Tree, 2: XGBoost, 3: SVC, 4: Random Forest}\"\"\"\n",
    "\n",
    "    all_models = [LogisticRegression(), DecisionTreeClassifier(random_state=91), XGBClassifier(random_state=0),\n",
    "                 SVC(random_state=0, kernel='linear'), RandomForestClassifier(random_state=0)]\n",
    "\n",
    "    # converting into numpy array\n",
    "    trainx, testx, trainy, testy = trainx.values, testx.values, trainy.values, testy.values\n",
    "\n",
    "    # train all models \n",
    "    if predictor == -1:\n",
    "        for model in all_models:\n",
    "            name = model.__class__.__name__\n",
    "\n",
    "            # pipeline\n",
    "            model_pipe = Pipeline([('Scaler', StandardScaler()), ('model', model)])\n",
    "\n",
    "            # fit predict\n",
    "            model_pipe.fit(trainx, trainy)\n",
    "            print(f\"{'='*20} {name} {'='*20}\")\n",
    "            print(classification_report(testy, model_pipe.predict(testx)))\n",
    "            print('Training score: ' + str(model_pipe.score(trainx, trainy)))\n",
    "            print('Test score: ' + str(model_pipe.score(testx, testy)), '\\n')\n",
    "            if cm:\n",
    "                plot_confusion_matrix(model_pipe, testx, testy, colorbar=False, cmap='binary')\n",
    "\n",
    "    # train selected models\n",
    "    else:\n",
    "        name = all_models[predictor].__class__.__name__\n",
    "\n",
    "        # pipeline\n",
    "        if parms:\n",
    "            model_pipe = Pipeline([('Scaler', StandardScaler()), ('model', all_models[predictor].set_params(**parms))])\n",
    "        else:\n",
    "            model_pipe = Pipeline([('Scaler', StandardScaler()), ('model', all_models[predictor])])\n",
    "\n",
    "        # fit predict\n",
    "        model_pipe.fit(trainx, trainy)\n",
    "        print(f\"{'='*20} {name} {'='*20}\")\n",
    "        print(classification_report(testy, model_pipe.predict(testx)))\n",
    "        print('Training score: ' + str(model_pipe.score(trainx, trainy)))\n",
    "        print('Test score: ' + str(model_pipe.score(testx, testy)), '\\n')\n",
    "        if cm:\n",
    "            plot_confusion_matrix(model_pipe, testx, testy, colorbar=False, cmap='binary')\n",
    "\n",
    "        # save it\n",
    "        if save:\n",
    "            filename = f\"{name}_model.p\"\n",
    "            joblib.dump(model_pipe, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model \n",
    "* All features were selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features & Label:  (4390, 17) (4390,)\n",
      "Train test split:  (3512, 17) (878, 17) (3512,) (878,)\n"
     ]
    }
   ],
   "source": [
    "# feature and label\n",
    "features = df.drop('stroke', axis=1)\n",
    "label = df['stroke']\n",
    "print('Features & Label: ', features.shape, label.shape)\n",
    "\n",
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=21)\n",
    "print('Train test split: ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       837\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.95       878\n",
      "   macro avg       0.48      0.50      0.49       878\n",
      "weighted avg       0.91      0.95      0.93       878\n",
      "\n",
      "Training score: 0.9646924829157175\n",
      "Test score: 0.9533029612756264 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       837\n",
      "           1       0.25      0.20      0.22        41\n",
      "\n",
      "    accuracy                           0.94       878\n",
      "   macro avg       0.61      0.58      0.59       878\n",
      "weighted avg       0.93      0.94      0.93       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9350797266514806 \n",
      "\n",
      "[00:06:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       837\n",
      "           1       0.21      0.07      0.11        41\n",
      "\n",
      "    accuracy                           0.94       878\n",
      "   macro avg       0.59      0.53      0.54       878\n",
      "weighted avg       0.92      0.94      0.93       878\n",
      "\n",
      "Training score: 0.9994305239179955\n",
      "Test score: 0.9441913439635535 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       837\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.95       878\n",
      "   macro avg       0.48      0.50      0.49       878\n",
      "weighted avg       0.91      0.95      0.93       878\n",
      "\n",
      "Training score: 0.9646924829157175\n",
      "Test score: 0.9533029612756264 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       837\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.95       878\n",
      "   macro avg       0.48      0.50      0.49       878\n",
      "weighted avg       0.91      0.95      0.93       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9510250569476082 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing on all models\n",
    "models(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Appart from Decision Tree and XGBoost no other model is able to classify stroke patient.\n",
    "* Decision tree's recall for stroke patient is 20 which is very low\n",
    "* XGBoost's recall for stroke patient is even lower than decision tree\n",
    "* We can not consider these model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Over and Under Sampling\n",
    "* All features\n",
    "* Over and Under sampled training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over:0.2, Under:0.7\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91       837\n",
      "           1       0.17      0.59      0.26        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.57      0.72      0.59       878\n",
      "weighted avg       0.94      0.84      0.88       878\n",
      "\n",
      "Training score: 0.8479318734793188\n",
      "Test score: 0.8428246013667426 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.13      0.37      0.19        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.55      0.62      0.56       878\n",
      "weighted avg       0.93      0.86      0.89       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.857630979498861 \n",
      "\n",
      "[00:06:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       837\n",
      "           1       0.17      0.32      0.22        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.57      0.62      0.58       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.89749430523918 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       837\n",
      "           1       0.17      0.63      0.27        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.58      0.74      0.59       878\n",
      "weighted avg       0.94      0.84      0.88       878\n",
      "\n",
      "Training score: 0.8582725060827251\n",
      "Test score: 0.8416856492027335 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       837\n",
      "           1       0.19      0.37      0.25        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.58      0.64      0.59       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8952164009111617 \n",
      "\n",
      "Over:0.2, Under:0.6\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.92       837\n",
      "           1       0.16      0.54      0.25        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.57      0.70      0.58       878\n",
      "weighted avg       0.94      0.85      0.88       878\n",
      "\n",
      "Training score: 0.8515235457063712\n",
      "Test score: 0.8485193621867881 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       837\n",
      "           1       0.14      0.41      0.21        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.55      0.64      0.56       878\n",
      "weighted avg       0.93      0.85      0.89       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8519362186788155 \n",
      "\n",
      "[00:06:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       837\n",
      "           1       0.18      0.32      0.23        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.57      0.62      0.59       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9020501138952164 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91       837\n",
      "           1       0.17      0.56      0.26        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.57      0.71      0.59       878\n",
      "weighted avg       0.94      0.85      0.88       878\n",
      "\n",
      "Training score: 0.8493074792243768\n",
      "Test score: 0.8473804100227791 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       837\n",
      "           1       0.19      0.34      0.24        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.58      0.63      0.59       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8997722095671982 \n",
      "\n",
      "Over:0.2, Under:0.5\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       837\n",
      "           1       0.18      0.49      0.26        41\n",
      "\n",
      "    accuracy                           0.87       878\n",
      "   macro avg       0.57      0.69      0.59       878\n",
      "weighted avg       0.94      0.87      0.90       878\n",
      "\n",
      "Training score: 0.8591826686361398\n",
      "Test score: 0.8690205011389521 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       837\n",
      "           1       0.16      0.37      0.22        41\n",
      "\n",
      "    accuracy                           0.88       878\n",
      "   macro avg       0.56      0.63      0.58       878\n",
      "weighted avg       0.93      0.88      0.90       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.878132118451025 \n",
      "\n",
      "[00:06:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       837\n",
      "           1       0.22      0.29      0.25        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.59      0.62      0.60       878\n",
      "weighted avg       0.93      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9179954441913439 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.17      0.51      0.25        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.57      0.69      0.59       878\n",
      "weighted avg       0.94      0.86      0.89       878\n",
      "\n",
      "Training score: 0.8537666174298375\n",
      "Test score: 0.8599088838268792 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       837\n",
      "           1       0.19      0.27      0.22        41\n",
      "\n",
      "    accuracy                           0.91       878\n",
      "   macro avg       0.57      0.61      0.59       878\n",
      "weighted avg       0.93      0.91      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9111617312072893 \n",
      "\n",
      "Over:0.3, Under:0.7\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91       837\n",
      "           1       0.15      0.51      0.23        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.56      0.69      0.57       878\n",
      "weighted avg       0.93      0.84      0.88       878\n",
      "\n",
      "Training score: 0.8552898256992298\n",
      "Test score: 0.8428246013667426 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       837\n",
      "           1       0.14      0.34      0.20        41\n",
      "\n",
      "    accuracy                           0.87       878\n",
      "   macro avg       0.55      0.62      0.56       878\n",
      "weighted avg       0.93      0.87      0.90       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8701594533029613 \n",
      "\n",
      "[00:06:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95       837\n",
      "           1       0.16      0.27      0.20        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.56      0.60      0.58       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9020501138952164 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91       837\n",
      "           1       0.16      0.54      0.25        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.57      0.70      0.58       878\n",
      "weighted avg       0.94      0.85      0.88       878\n",
      "\n",
      "Training score: 0.8638021888933928\n",
      "Test score: 0.8462414578587699 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       837\n",
      "           1       0.18      0.29      0.23        41\n",
      "\n",
      "    accuracy                           0.91       878\n",
      "   macro avg       0.57      0.61      0.59       878\n",
      "weighted avg       0.93      0.91      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9066059225512528 \n",
      "\n",
      "Over:0.3, Under:0.6\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       837\n",
      "           1       0.17      0.51      0.25        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.57      0.69      0.59       878\n",
      "weighted avg       0.94      0.86      0.89       878\n",
      "\n",
      "Training score: 0.8648947951273532\n",
      "Test score: 0.856492027334852 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       837\n",
      "           1       0.18      0.34      0.24        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.58      0.63      0.59       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8986332574031891 \n",
      "\n",
      "[00:06:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       837\n",
      "           1       0.20      0.29      0.24        41\n",
      "\n",
      "    accuracy                           0.91       878\n",
      "   macro avg       0.58      0.62      0.60       878\n",
      "weighted avg       0.93      0.91      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9123006833712984 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.92       837\n",
      "           1       0.16      0.54      0.25        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.57      0.70      0.58       878\n",
      "weighted avg       0.94      0.85      0.89       878\n",
      "\n",
      "Training score: 0.8623108157991879\n",
      "Test score: 0.8496583143507973 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       837\n",
      "           1       0.22      0.29      0.25        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.59      0.62      0.60       878\n",
      "weighted avg       0.93      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9191343963553531 \n",
      "\n",
      "Over:0.3, Under:0.5\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.16      0.46      0.24        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.57      0.67      0.58       878\n",
      "weighted avg       0.93      0.86      0.89       878\n",
      "\n",
      "Training score: 0.8677821522309711\n",
      "Test score: 0.8633257403189066 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       837\n",
      "           1       0.09      0.17      0.12        41\n",
      "\n",
      "    accuracy                           0.88       878\n",
      "   macro avg       0.52      0.54      0.53       878\n",
      "weighted avg       0.92      0.88      0.90       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8815489749430524 \n",
      "\n",
      "[00:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       837\n",
      "           1       0.29      0.27      0.28        41\n",
      "\n",
      "    accuracy                           0.94       878\n",
      "   macro avg       0.63      0.62      0.62       878\n",
      "weighted avg       0.93      0.94      0.93       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9350797266514806 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.93       837\n",
      "           1       0.18      0.51      0.26        41\n",
      "\n",
      "    accuracy                           0.87       878\n",
      "   macro avg       0.58      0.70      0.60       878\n",
      "weighted avg       0.94      0.87      0.90       878\n",
      "\n",
      "Training score: 0.8664698162729659\n",
      "Test score: 0.8667425968109339 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       837\n",
      "           1       0.16      0.20      0.18        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.56      0.57      0.57       878\n",
      "weighted avg       0.92      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9157175398633257 \n",
      "\n",
      "Over:0.4, Under:0.7\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91       837\n",
      "           1       0.15      0.51      0.24        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.56      0.69      0.57       878\n",
      "weighted avg       0.93      0.85      0.88       878\n",
      "\n",
      "Training score: 0.8662613981762918\n",
      "Test score: 0.8451025056947609 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       837\n",
      "           1       0.14      0.27      0.18        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.55      0.59      0.56       878\n",
      "weighted avg       0.92      0.89      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8895216400911162 \n",
      "\n",
      "[00:06:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       837\n",
      "           1       0.19      0.27      0.22        41\n",
      "\n",
      "    accuracy                           0.91       878\n",
      "   macro avg       0.57      0.61      0.59       878\n",
      "weighted avg       0.93      0.91      0.92       878\n",
      "\n",
      "Training score: 0.9996960486322188\n",
      "Test score: 0.9111617312072893 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       837\n",
      "           1       0.17      0.56      0.26        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.57      0.71      0.59       878\n",
      "weighted avg       0.94      0.85      0.89       878\n",
      "\n",
      "Training score: 0.8699088145896656\n",
      "Test score: 0.8496583143507973 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       837\n",
      "           1       0.20      0.24      0.22        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.58      0.60      0.59       878\n",
      "weighted avg       0.93      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9179954441913439 \n",
      "\n",
      "Over:0.4, Under:0.6\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.15      0.46      0.23        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.56      0.67      0.58       878\n",
      "weighted avg       0.93      0.86      0.89       878\n",
      "\n",
      "Training score: 0.866592859119845\n",
      "Test score: 0.856492027334852 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       837\n",
      "           1       0.13      0.24      0.17        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.54      0.58      0.55       878\n",
      "weighted avg       0.92      0.89      0.90       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8861047835990888 \n",
      "\n",
      "[00:06:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       837\n",
      "           1       0.20      0.20      0.20        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.58      0.58      0.58       878\n",
      "weighted avg       0.92      0.92      0.92       878\n",
      "\n",
      "Training score: 0.9997232216994187\n",
      "Test score: 0.9248291571753986 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.17      0.51      0.25        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.57      0.69      0.59       878\n",
      "weighted avg       0.94      0.86      0.89       878\n",
      "\n",
      "Training score: 0.8629947412122889\n",
      "Test score: 0.8599088838268792 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       837\n",
      "           1       0.17      0.17      0.17        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.56      0.56      0.56       878\n",
      "weighted avg       0.92      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9214123006833713 \n",
      "\n",
      "Over:0.4, Under:0.5\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       837\n",
      "           1       0.16      0.41      0.23        41\n",
      "\n",
      "    accuracy                           0.87       878\n",
      "   macro avg       0.56      0.65      0.58       878\n",
      "weighted avg       0.93      0.87      0.90       878\n",
      "\n",
      "Training score: 0.8676506765067651\n",
      "Test score: 0.8690205011389521 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       837\n",
      "           1       0.14      0.24      0.17        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.55      0.58      0.56       878\n",
      "weighted avg       0.92      0.89      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8917995444191344 \n",
      "\n",
      "[00:06:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       837\n",
      "           1       0.20      0.17      0.18        41\n",
      "\n",
      "    accuracy                           0.93       878\n",
      "   macro avg       0.58      0.57      0.57       878\n",
      "weighted avg       0.92      0.93      0.93       878\n",
      "\n",
      "Training score: 0.9995079950799508\n",
      "Test score: 0.929384965831435 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       837\n",
      "           1       0.16      0.41      0.23        41\n",
      "\n",
      "    accuracy                           0.87       878\n",
      "   macro avg       0.57      0.66      0.58       878\n",
      "weighted avg       0.93      0.87      0.90       878\n",
      "\n",
      "Training score: 0.8691266912669127\n",
      "Test score: 0.8735763097949886 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       837\n",
      "           1       0.18      0.15      0.16        41\n",
      "\n",
      "    accuracy                           0.93       878\n",
      "   macro avg       0.57      0.56      0.56       878\n",
      "weighted avg       0.92      0.93      0.93       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.929384965831435 \n",
      "\n",
      "Over:0.5, Under:0.7\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       837\n",
      "           1       0.15      0.46      0.22        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.56      0.67      0.57       878\n",
      "weighted avg       0.93      0.85      0.88       878\n",
      "\n",
      "Training score: 0.8684978123480798\n",
      "Test score: 0.8485193621867881 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       837\n",
      "           1       0.10      0.17      0.13        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.53      0.55      0.54       878\n",
      "weighted avg       0.92      0.89      0.90       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8917995444191344 \n",
      "\n",
      "[00:06:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       837\n",
      "           1       0.19      0.20      0.19        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.58      0.58      0.58       878\n",
      "weighted avg       0.92      0.92      0.92       878\n",
      "\n",
      "Training score: 0.9990277102576568\n",
      "Test score: 0.9236902050113895 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92       837\n",
      "           1       0.17      0.54      0.26        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.57      0.70      0.59       878\n",
      "weighted avg       0.94      0.86      0.89       878\n",
      "\n",
      "Training score: 0.87311618862421\n",
      "Test score: 0.8553530751708428 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       837\n",
      "           1       0.14      0.15      0.14        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.55      0.55      0.55       878\n",
      "weighted avg       0.92      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9191343963553531 \n",
      "\n",
      "Over:0.5, Under:0.6\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.16      0.44      0.23        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.56      0.66      0.58       878\n",
      "weighted avg       0.93      0.86      0.89       878\n",
      "\n",
      "Training score: 0.8658401593978304\n",
      "Test score: 0.8633257403189066 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       837\n",
      "           1       0.11      0.20      0.14        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.54      0.56      0.54       878\n",
      "weighted avg       0.92      0.89      0.90       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8906605922551253 \n",
      "\n",
      "[00:07:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       837\n",
      "           1       0.18      0.15      0.16        41\n",
      "\n",
      "    accuracy                           0.93       878\n",
      "   macro avg       0.57      0.56      0.56       878\n",
      "weighted avg       0.92      0.93      0.93       878\n",
      "\n",
      "Training score: 0.9988930706220943\n",
      "Test score: 0.928246013667426 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93       837\n",
      "           1       0.18      0.49      0.26        41\n",
      "\n",
      "    accuracy                           0.87       878\n",
      "   macro avg       0.58      0.69      0.60       878\n",
      "weighted avg       0.94      0.87      0.90       878\n",
      "\n",
      "Training score: 0.8647332300199247\n",
      "Test score: 0.8712984054669703 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       837\n",
      "           1       0.14      0.12      0.13        41\n",
      "\n",
      "    accuracy                           0.92       878\n",
      "   macro avg       0.55      0.54      0.54       878\n",
      "weighted avg       0.92      0.92      0.92       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9236902050113895 \n",
      "\n",
      "Over:0.5, Under:0.5\n",
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       837\n",
      "           1       0.16      0.39      0.23        41\n",
      "\n",
      "    accuracy                           0.88       878\n",
      "   macro avg       0.57      0.65      0.58       878\n",
      "weighted avg       0.93      0.88      0.90       878\n",
      "\n",
      "Training score: 0.8713105076741441\n",
      "Test score: 0.878132118451025 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       837\n",
      "           1       0.12      0.20      0.15        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.54      0.56      0.54       878\n",
      "weighted avg       0.92      0.89      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8929384965831435 \n",
      "\n",
      "[00:07:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       837\n",
      "           1       0.22      0.15      0.18        41\n",
      "\n",
      "    accuracy                           0.94       878\n",
      "   macro avg       0.59      0.56      0.57       878\n",
      "weighted avg       0.92      0.94      0.93       878\n",
      "\n",
      "Training score: 0.9982290436835891\n",
      "Test score: 0.9362186788154897 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93       837\n",
      "           1       0.15      0.34      0.20        41\n",
      "\n",
      "    accuracy                           0.88       878\n",
      "   macro avg       0.56      0.62      0.57       878\n",
      "weighted avg       0.93      0.88      0.90       878\n",
      "\n",
      "Training score: 0.8744588744588745\n",
      "Test score: 0.8758542141230068 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       837\n",
      "           1       0.18      0.12      0.14        41\n",
      "\n",
      "    accuracy                           0.93       878\n",
      "   macro avg       0.57      0.55      0.55       878\n",
      "weighted avg       0.92      0.93      0.93       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.9328018223234624 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best ratio for over and undersampling\n",
    "\n",
    "# values to evaluate\n",
    "over_values = [0.2,0.3,0.4,0.5]\n",
    "under_values = [0.7,0.6,0.5]\n",
    "\n",
    "\n",
    "for o in over_values:\n",
    "  for u in under_values:\n",
    "        # resampling\n",
    "        over = SMOTE(sampling_strategy=o, k_neighbors=2, random_state=21)\n",
    "        under = RandomUnderSampler(sampling_strategy=u, random_state=21)\n",
    "\n",
    "        balancer = imb_pipeline([\n",
    "            ('Over', over),\n",
    "            ('Under', under)])\n",
    "\n",
    "        x_train_, y_train_ = balancer.fit_resample(x_train, y_train)\n",
    "\n",
    "        # predicting\n",
    "        print(f\"Over:{o}, Under:{u}\")\n",
    "        models(x_train_, x_test, y_train_, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Its turn out that our best model have achived 0.63 value of `Recall of class 1` \n",
    "* We achived best Recall for class 1 when the ratio of SMOTE's Oversampling is 0.2, and Undersampling is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote for balancing class labels\n",
    "over = SMOTE(sampling_strategy=0.2, k_neighbors=2, random_state=21)\n",
    "under = RandomUnderSampler(sampling_strategy=0.7, random_state=21)\n",
    "\n",
    "balancer = imb_pipeline([\n",
    "    ('Over', over),\n",
    "    ('Under', under)])\n",
    "\n",
    "x_train, y_train = balancer.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91       837\n",
      "           1       0.17      0.59      0.26        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.57      0.72      0.59       878\n",
      "weighted avg       0.94      0.84      0.88       878\n",
      "\n",
      "Training score: 0.8479318734793188\n",
      "Test score: 0.8428246013667426 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.13      0.37      0.19        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.55      0.62      0.56       878\n",
      "weighted avg       0.93      0.86      0.89       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.857630979498861 \n",
      "\n",
      "[00:07:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       837\n",
      "           1       0.17      0.32      0.22        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.57      0.62      0.58       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.89749430523918 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       837\n",
      "           1       0.17      0.63      0.27        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.58      0.74      0.59       878\n",
      "weighted avg       0.94      0.84      0.88       878\n",
      "\n",
      "Training score: 0.8582725060827251\n",
      "Test score: 0.8416856492027335 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       837\n",
      "           1       0.19      0.37      0.25        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.58      0.64      0.59       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8952164009111617 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* After data sampling, We have achived highest recall for stroke patient is 0.63.\n",
    "* SVM classifier is has the higest recall compared to other model but seems little overfitted\n",
    "* Logistic regression has the 2nd higest recall of 0.59 and also seems generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with Age binned column\n",
    "* All features\n",
    "* Over and Under sampled data\n",
    "* Age column is binned between 4 groups \"Toddlers\", \"Young\", \"Elder\", and \"Senior\"\n",
    "* Age breakpoints are as follows 5, 18, 40, 40+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column binner\n",
    "def create_bins(col, cut_points, labels=None):\n",
    "    minval = col.min()\n",
    "    maxval = col.max()\n",
    "    break_points = [minval] + cut_points+ [maxval]\n",
    "    print(break_points)\n",
    "\n",
    "    if not labels:\n",
    "        labels = range(len(cut_points) +1)\n",
    "\n",
    "    colBin = pd.cut(col, bins=break_points, labels=labels, include_lowest=True)\n",
    "    return colBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08, 5, 18, 40, 82.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>work_type_Govt_job</th>\n",
       "      <th>work_type_Never_worked</th>\n",
       "      <th>...</th>\n",
       "      <th>work_type_Self-employed</th>\n",
       "      <th>work_type_children</th>\n",
       "      <th>smoking_status_Unknown</th>\n",
       "      <th>smoking_status_formerly smoked</th>\n",
       "      <th>smoking_status_never smoked</th>\n",
       "      <th>smoking_status_smokes</th>\n",
       "      <th>AgeGrp_Toddlers</th>\n",
       "      <th>AgeGrp_Young</th>\n",
       "      <th>AgeGrp_Elder</th>\n",
       "      <th>AgeGrp_Senior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.662684</td>\n",
       "      <td>3.481240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.249780</td>\n",
       "      <td>3.310543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.547435</td>\n",
       "      <td>3.126761</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.332705</td>\n",
       "      <td>3.335770</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.070223</td>\n",
       "      <td>3.186353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease  ever_married  Residence_type  \\\n",
       "0       1  80.0             0              1             0               0   \n",
       "1       1  74.0             1              1             0               0   \n",
       "2       0  69.0             0              0             0               0   \n",
       "3       0  59.0             0              0             0               0   \n",
       "4       0  78.0             0              0             0               0   \n",
       "\n",
       "   avg_glucose_level       bmi  work_type_Govt_job  work_type_Never_worked  \\\n",
       "0           4.662684  3.481240                   0                       0   \n",
       "1           4.249780  3.310543                   0                       0   \n",
       "2           4.547435  3.126761                   0                       0   \n",
       "3           4.332705  3.335770                   0                       0   \n",
       "4           4.070223  3.186353                   0                       0   \n",
       "\n",
       "   ...  work_type_Self-employed  work_type_children  smoking_status_Unknown  \\\n",
       "0  ...                        0                   0                       0   \n",
       "1  ...                        0                   0                       0   \n",
       "2  ...                        0                   0                       0   \n",
       "3  ...                        0                   0                       1   \n",
       "4  ...                        0                   0                       1   \n",
       "\n",
       "   smoking_status_formerly smoked  smoking_status_never smoked  \\\n",
       "0                               0                            1   \n",
       "1                               0                            1   \n",
       "2                               0                            1   \n",
       "3                               0                            0   \n",
       "4                               0                            0   \n",
       "\n",
       "   smoking_status_smokes  AgeGrp_Toddlers  AgeGrp_Young  AgeGrp_Elder  \\\n",
       "0                      0                0             0             0   \n",
       "1                      0                0             0             0   \n",
       "2                      0                0             0             0   \n",
       "3                      0                0             0             0   \n",
       "4                      0                0             0             0   \n",
       "\n",
       "   AgeGrp_Senior  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binning age with 4 groups\n",
    "cut_points = [5, 18, 40];\n",
    "labels= [\"Toddlers\", \"Young\", \"Elder\", \"Senior\"]\n",
    "features_bin = features.copy()\n",
    "features_bin[\"AgeGrp\"] = create_bins(features_bin[\"age\"], cut_points, labels)\n",
    "\n",
    "# one hot encoding\n",
    "features_bin = pd.get_dummies(features_bin, columns=['AgeGrp'])\n",
    "features_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features & Label:  (4390, 21) (4390,)\n",
      "Train test split:  (3512, 21) (878, 21) (3512,) (878,)\n",
      "Smote:  (1644, 21) (1644,)\n"
     ]
    }
   ],
   "source": [
    "# feature and label\n",
    "print('Features & Label: ', features_bin.shape, label.shape)\n",
    "\n",
    "# train test split\n",
    "x_train_bin, x_test_bin, y_train_bin, y_test_bin = train_test_split(features_bin, label, test_size=0.2, random_state=21)\n",
    "print('Train test split: ', x_train_bin.shape, x_test_bin.shape, y_train_bin.shape, y_test_bin.shape)\n",
    "\n",
    "# Smote\n",
    "x_train_bin, y_train_bin = balancer.fit_resample(x_train_bin, y_train_bin)\n",
    "print(\"Smote: \", x_train_bin.shape, y_train_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91       837\n",
      "           1       0.17      0.59      0.26        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.57      0.72      0.59       878\n",
      "weighted avg       0.94      0.84      0.88       878\n",
      "\n",
      "Training score: 0.8485401459854015\n",
      "Test score: 0.8428246013667426 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       837\n",
      "           1       0.14      0.41      0.21        41\n",
      "\n",
      "    accuracy                           0.86       878\n",
      "   macro avg       0.56      0.65      0.57       878\n",
      "weighted avg       0.93      0.86      0.89       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8553530751708428 \n",
      "\n",
      "[00:07:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       837\n",
      "           1       0.18      0.32      0.23        41\n",
      "\n",
      "    accuracy                           0.90       878\n",
      "   macro avg       0.57      0.62      0.59       878\n",
      "weighted avg       0.93      0.90      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8986332574031891 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91       837\n",
      "           1       0.18      0.63      0.28        41\n",
      "\n",
      "    accuracy                           0.85       878\n",
      "   macro avg       0.58      0.74      0.59       878\n",
      "weighted avg       0.94      0.85      0.88       878\n",
      "\n",
      "Training score: 0.8576642335766423\n",
      "Test score: 0.8451025056947609 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94       837\n",
      "           1       0.20      0.44      0.27        41\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.59      0.68      0.61       878\n",
      "weighted avg       0.93      0.89      0.91       878\n",
      "\n",
      "Training score: 1.0\n",
      "Test score: 0.8917995444191344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models(x_train_bin, x_test_bin, y_train_bin, y_test_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Age binning isn't seems to be improving the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with feature selection\n",
    "* Selected features\n",
    "* Over and Under sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finding relevat column by information gain\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# score of the dataset\n",
    "score = mutual_info_classif(df.drop('stroke', axis=1), df['stroke'], random_state=21)\n",
    "\n",
    "# lets plot it\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(df.drop('stroke', axis=1).columns, score, color = 'cyan', edgecolor='black')\n",
    "plt.axvline(0.005, color='red')\n",
    "plt.title('Importance of Feature')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* According to Information gain's score with theshold 0.005 following features are good stroke predictor\n",
    "    * ['age', 'hypertension', 'bmi', 'work_type_Self-employed', 'work_type_children']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split:  (3512, 5) (878, 5) (3512,) (878,)\n",
      "Smote:  (1644, 5) (1644,)\n"
     ]
    }
   ],
   "source": [
    "col = ['age', 'hypertension', 'bmi', 'work_type_Self-employed', 'work_type_children']\n",
    "features_ig = features[col]\n",
    "\n",
    "# train test split\n",
    "x_train_ig, x_test_ig, y_train_ig, y_test_ig = train_test_split(features_ig, label, test_size=0.2, random_state=21)\n",
    "print('Train test split: ', x_train_ig.shape, x_test_ig.shape, y_train_ig.shape, y_test_ig.shape)\n",
    "\n",
    "# Smote\n",
    "x_train_ig, y_train_ig = balancer.fit_resample(x_train_ig, y_train_ig)\n",
    "print(\"Smote: \", x_train_ig.shape, y_train_ig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87       837\n",
      "           1       0.14      0.71      0.23        41\n",
      "\n",
      "    accuracy                           0.78       878\n",
      "   macro avg       0.56      0.74      0.55       878\n",
      "weighted avg       0.94      0.78      0.84       878\n",
      "\n",
      "Training score: 0.7652068126520681\n",
      "Test score: 0.7767653758542141 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90       837\n",
      "           1       0.12      0.44      0.19        41\n",
      "\n",
      "    accuracy                           0.82       878\n",
      "   macro avg       0.54      0.64      0.55       878\n",
      "weighted avg       0.93      0.82      0.87       878\n",
      "\n",
      "Training score: 0.995742092457421\n",
      "Test score: 0.8246013667425968 \n",
      "\n",
      "[00:07:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       837\n",
      "           1       0.12      0.41      0.19        41\n",
      "\n",
      "    accuracy                           0.83       878\n",
      "   macro avg       0.54      0.63      0.55       878\n",
      "weighted avg       0.93      0.83      0.87       878\n",
      "\n",
      "Training score: 0.968978102189781\n",
      "Test score: 0.8314350797266514 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       837\n",
      "           1       0.13      0.71      0.22        41\n",
      "\n",
      "    accuracy                           0.77       878\n",
      "   macro avg       0.56      0.74      0.54       878\n",
      "weighted avg       0.94      0.77      0.83       878\n",
      "\n",
      "Training score: 0.7603406326034063\n",
      "Test score: 0.765375854214123 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       837\n",
      "           1       0.14      0.49      0.22        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.56      0.67      0.56       878\n",
      "weighted avg       0.93      0.84      0.88       878\n",
      "\n",
      "Training score: 0.9951338199513382\n",
      "Test score: 0.837129840546697 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models(x_train_ig, x_test_ig, y_train_ig, y_test_ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Feature selection did improved the performance of the model\n",
    "* Logistic regression and Support vector classifier are having same recall score of 0.71 for stroke class\n",
    "* However Logistic regression has little better precision and more generalized than support vector classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with Feature selection and Age binning\n",
    "* Selected Features\n",
    "* Over and Under sampled data\n",
    "* Age binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test split:  (3512, 5) (878, 5) (3512,) (878,)\n",
      "Smote:  (1644, 5) (1644,)\n"
     ]
    }
   ],
   "source": [
    "col = ['age', 'hypertension', 'bmi', 'work_type_Self-employed', 'work_type_children']\n",
    "features_bin_ig = features_bin[col]\n",
    "\n",
    "# train test split\n",
    "x_train_bin_ig, x_test_bin_ig, y_train_bin_ig, y_test_bin_ig = train_test_split(features_bin_ig, label, test_size=0.2, random_state=21)\n",
    "print('Train test split: ', x_train_bin_ig.shape, x_test_bin_ig.shape, y_train_bin_ig.shape, y_test_bin_ig.shape)\n",
    "\n",
    "# Smote\n",
    "x_train_bin_ig, y_train_bin_ig = balancer.fit_resample(x_train_bin_ig, y_train_bin_ig)\n",
    "print(\"Smote: \", x_train_bin_ig.shape, y_train_bin_ig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87       837\n",
      "           1       0.14      0.71      0.23        41\n",
      "\n",
      "    accuracy                           0.78       878\n",
      "   macro avg       0.56      0.74      0.55       878\n",
      "weighted avg       0.94      0.78      0.84       878\n",
      "\n",
      "Training score: 0.7652068126520681\n",
      "Test score: 0.7767653758542141 \n",
      "\n",
      "==================== DecisionTreeClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90       837\n",
      "           1       0.12      0.44      0.19        41\n",
      "\n",
      "    accuracy                           0.82       878\n",
      "   macro avg       0.54      0.64      0.55       878\n",
      "weighted avg       0.93      0.82      0.87       878\n",
      "\n",
      "Training score: 0.995742092457421\n",
      "Test score: 0.8246013667425968 \n",
      "\n",
      "[00:07:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "==================== XGBClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       837\n",
      "           1       0.12      0.41      0.19        41\n",
      "\n",
      "    accuracy                           0.83       878\n",
      "   macro avg       0.54      0.63      0.55       878\n",
      "weighted avg       0.93      0.83      0.87       878\n",
      "\n",
      "Training score: 0.968978102189781\n",
      "Test score: 0.8314350797266514 \n",
      "\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       837\n",
      "           1       0.13      0.71      0.22        41\n",
      "\n",
      "    accuracy                           0.77       878\n",
      "   macro avg       0.56      0.74      0.54       878\n",
      "weighted avg       0.94      0.77      0.83       878\n",
      "\n",
      "Training score: 0.7603406326034063\n",
      "Test score: 0.765375854214123 \n",
      "\n",
      "==================== RandomForestClassifier ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91       837\n",
      "           1       0.14      0.49      0.22        41\n",
      "\n",
      "    accuracy                           0.84       878\n",
      "   macro avg       0.56      0.67      0.56       878\n",
      "weighted avg       0.93      0.84      0.88       878\n",
      "\n",
      "Training score: 0.9951338199513382\n",
      "Test score: 0.837129840546697 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models(x_train_bin_ig, x_test_bin_ig, y_train_bin_ig, y_test_bin_ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Recall seems to be same as the model with only features selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== LogisticRegression ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.85       837\n",
      "           1       0.13      0.78      0.23        41\n",
      "\n",
      "    accuracy                           0.75       878\n",
      "   macro avg       0.56      0.77      0.54       878\n",
      "weighted avg       0.95      0.75      0.82       878\n",
      "\n",
      "Training score: 0.7688564476885644\n",
      "Test score: 0.7528473804100227 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parms = dict(class_weight={0:45, 1:55}, n_jobs=-1)\n",
    "models(x_train_ig, x_test_ig, y_train_ig, y_test_ig, parms=parms, predictor=0, cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Recall of stroke class is slightly improved after introducing class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.70      0.82       837\n",
      "           1       0.12      0.85      0.21        41\n",
      "\n",
      "    accuracy                           0.71       878\n",
      "   macro avg       0.56      0.78      0.52       878\n",
      "weighted avg       0.95      0.71      0.79       878\n",
      "\n",
      "Training score: 0.7852798053527981\n",
      "Test score: 0.7050113895216401 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parms = dict(class_weight={0:45, 1:55}, kernel='poly')\n",
    "# models(x_train_ig, x_test_ig, y_train_ig, y_test_ig, parms=parms, predictor=3, cm=True)\n",
    "models(x_train_ig, x_test_ig, y_train_ig, y_test_ig, parms=parms, predictor=3, cm=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Introducing class weight and selecting 'poly' kernal improved the stroke class recall to 85\n",
    "* Recall of non-stroke class is also reduced to 70 which is probably not a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.58      0.73       837\n",
      "           1       0.10      0.90      0.17        41\n",
      "\n",
      "    accuracy                           0.59       878\n",
      "   macro avg       0.54      0.74      0.45       878\n",
      "weighted avg       0.95      0.59      0.71       878\n",
      "\n",
      "Training score: 0.7463503649635036\n",
      "Test score: 0.5945330296127562 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase stroke class weightage to 65\n",
    "parms = dict(class_weight={0:35, 1:65}, kernel='poly', )\n",
    "models(x_train_ig, x_test_ig, y_train_ig, y_test_ig, parms=parms, predictor=3, cm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* High weightage to stroke class increase it's recall but also slightly decrease the recall of non-stroke class which in turn decrease the performance of overall model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.71      0.83       845\n",
      "           1       0.10      0.79      0.17        33\n",
      "\n",
      "    accuracy                           0.71       878\n",
      "   macro avg       0.54      0.75      0.50       878\n",
      "weighted avg       0.95      0.71      0.80       878\n",
      "\n",
      "Training score: 0.7708714198659354\n",
      "Test score: 0.714123006833713 \n",
      "\n",
      "fold: 1\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.75      0.85       836\n",
      "           1       0.12      0.71      0.21        42\n",
      "\n",
      "    accuracy                           0.75       878\n",
      "   macro avg       0.55      0.73      0.53       878\n",
      "weighted avg       0.94      0.75      0.82       878\n",
      "\n",
      "Training score: 0.7931873479318735\n",
      "Test score: 0.7460136674259681 \n",
      "\n",
      "fold: 2\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84       849\n",
      "           1       0.10      0.86      0.18        29\n",
      "\n",
      "    accuracy                           0.74       878\n",
      "   macro avg       0.55      0.80      0.51       878\n",
      "weighted avg       0.96      0.74      0.82       878\n",
      "\n",
      "Training score: 0.7577791336180598\n",
      "Test score: 0.7380410022779044 \n",
      "\n",
      "fold: 3\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.72      0.83       846\n",
      "           1       0.09      0.69      0.15        32\n",
      "\n",
      "    accuracy                           0.72       878\n",
      "   macro avg       0.54      0.71      0.49       878\n",
      "weighted avg       0.95      0.72      0.81       878\n",
      "\n",
      "Training score: 0.7803538743136058\n",
      "Test score: 0.7232346241457859 \n",
      "\n",
      "fold: 4\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.73      0.84       849\n",
      "           1       0.09      0.79      0.16        29\n",
      "\n",
      "    accuracy                           0.73       878\n",
      "   macro avg       0.54      0.76      0.50       878\n",
      "weighted avg       0.96      0.73      0.82       878\n",
      "\n",
      "Training score: 0.7949969493593655\n",
      "Test score: 0.7289293849658315 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kfold\n",
    "kf_cv = KFold(n_splits=5, shuffle=True) \n",
    "\n",
    "# splitting data with KFold\n",
    "scores = []\n",
    "i = 0\n",
    "for train_index, test_index in kf_cv.split(features_ig):\n",
    "    trainx, testx, trainy, testy = features_ig.iloc[train_index], features_ig.iloc[test_index], label[train_index], label[test_index]\n",
    "    trainx, trainy = balancer.fit_resample(trainx, trainy)\n",
    "    parms = dict(class_weight={0:45, 1:55}, kernel='poly')\n",
    "    print(f'fold: {i}')\n",
    "    models(trainx, testx, trainy, testy, parms=parms, predictor=3)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Recall for stroke class throughout different fold was as follow;\n",
    "    * {Fold0: 0.74, Fold1:0.79, Fold2:0.80, Fold3:58, Fold4:0.67}\n",
    "* Average recall seen was 0.716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.77      0.86       845\n",
      "           1       0.11      0.70      0.18        33\n",
      "\n",
      "    accuracy                           0.77       878\n",
      "   macro avg       0.55      0.73      0.52       878\n",
      "weighted avg       0.95      0.77      0.84       878\n",
      "\n",
      "Training score: 0.7720901889092017\n",
      "Test score: 0.7665148063781321 \n",
      "\n",
      "fold: 1\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88       842\n",
      "           1       0.13      0.75      0.23        36\n",
      "\n",
      "    accuracy                           0.79       878\n",
      "   macro avg       0.56      0.77      0.55       878\n",
      "weighted avg       0.95      0.79      0.85       878\n",
      "\n",
      "Training score: 0.7458866544789763\n",
      "Test score: 0.7892938496583144 \n",
      "\n",
      "fold: 2\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87       852\n",
      "           1       0.10      0.77      0.17        26\n",
      "\n",
      "    accuracy                           0.78       878\n",
      "   macro avg       0.54      0.77      0.52       878\n",
      "weighted avg       0.96      0.78      0.85       878\n",
      "\n",
      "Training score: 0.7579462102689487\n",
      "Test score: 0.7801822323462415 \n",
      "\n",
      "fold: 3\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88       845\n",
      "           1       0.11      0.67      0.19        33\n",
      "\n",
      "    accuracy                           0.78       878\n",
      "   macro avg       0.55      0.73      0.53       878\n",
      "weighted avg       0.95      0.78      0.85       878\n",
      "\n",
      "Training score: 0.7580743449116393\n",
      "Test score: 0.7835990888382688 \n",
      "\n",
      "fold: 4\n",
      "==================== SVC ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88       841\n",
      "           1       0.13      0.68      0.21        37\n",
      "\n",
      "    accuracy                           0.79       878\n",
      "   macro avg       0.55      0.74      0.55       878\n",
      "weighted avg       0.95      0.79      0.85       878\n",
      "\n",
      "Training score: 0.7672151127361365\n",
      "Test score: 0.7915717539863326 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kfold\n",
    "kf_cv = KFold(n_splits=5, shuffle=True) \n",
    "\n",
    "# splitting data with KFold\n",
    "scores = []\n",
    "i = 0\n",
    "for train_index, test_index in kf_cv.split(features_ig):\n",
    "    trainx, testx, trainy, testy = features_ig.iloc[train_index], features_ig.iloc[test_index], label[train_index], label[test_index]\n",
    "    trainx, trainy = balancer.fit_resample(trainx, trainy)\n",
    "    parms = dict(class_weight={0:45, 1:55}, kernel='poly')\n",
    "    print(f'fold: {i}')\n",
    "    models(trainx, testx, trainy, testy, predictor=3)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange'>**Note**</font>\n",
    "* Recall for stroke class throughout different fold was as follow;\n",
    "    * {Fold0:0.70, Fold1:0.64, Fold2:0.70, Fold3:0.64, Fold4:0.73}\n",
    "* Average recall seen was 0.716"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "591d334517c5d70a2bae2c9c28377b8500948e485d060e75b3e79d2166d8783a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
